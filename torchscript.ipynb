{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5776319",
   "metadata": {},
   "source": [
    "### Colab Setting\n",
    "- torchscript_inference.ipynb, train.ipynb 의 device와 동일하게 설정 필수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01ce3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "device = 'cuda'\n",
    "target_weight = 'checkpoint/weights-0.pt' # torchscript 로 바꿀 가중치 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/bridgeblip\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044fc3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import InstructBlipForConditionalGeneration, InstructBlipProcessor, InstructBlipConfig\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d15f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707a4bcc",
   "metadata": {},
   "source": [
    "### Create **LoRA-Bridge** InstructBlip - load within 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc3335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'total params      : {total_params:,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488bb29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout):\n",
    "        super().__init__()\n",
    "        self.layer= nn.Sequential(\n",
    "            nn.Linear(in_features, in_features),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features, out_features)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d75071",
   "metadata": {},
   "source": [
    "- instructblip.config.json 은 Huggingface - Salesforce/instructblip-flan-t5-xl 의 config.json 을 아래와 같이 수정한 파일입니다.\n",
    "\n",
    "- change: **text_config.num_decoder_layers** 24 -> 8\n",
    "- change: **text_config.vocab_size** 32128 -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52765378",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\", use_fast=True)\n",
    "base_blip = InstructBlipForConditionalGeneration(InstructBlipConfig.from_json_file('instructblip.config.json'))\n",
    "\n",
    "\n",
    "# transformers.models.t5.modeling_t5.T5Model 을 통해 내부 코드를 확인하면 \n",
    "# shared, encoder.embed_tokens, decoder.embed_tokens 는 공유 파라메터 구조 입니다.\n",
    "\n",
    "# 처음 로드될때 불필요하게 lm_head 까지 nn.Linear(2048, 32128) 로 큰 파라메터로 로드 되는것을 막기 위해 vocab_size = 0 으로 설정후 \n",
    "# encoder, decoder 부분을 원래 instructblip 구조 대로 복원하는 과정입니다. \n",
    "\n",
    "\n",
    "# nn.Embedding(0, 2048) to nn.Embedding(32128, 2048)\n",
    "base_blip.language_model.shared = nn.Embedding(32128, 2048)\n",
    "base_blip.language_model.encoder.embed_tokens = base_blip.language_model.shared\n",
    "base_blip.language_model.decoder.embed_tokens = base_blip.language_model.shared\n",
    "\n",
    "\n",
    "# nn.Linear(2048, 0) to ClassificationHead\n",
    "base_blip.language_model.lm_head = ClassificationHead(\n",
    "    in_features=base_blip.language_model.config.d_model,\n",
    "    out_features=4,\n",
    "    dropout=0.0\n",
    ")\n",
    "\n",
    "\n",
    "# decoder total 585,944,064 to 604,818,432\n",
    "lora_config = LoraConfig(\n",
    "    r=96,\n",
    "    lora_alpha=192,\n",
    "    target_modules=['q', 'k', 'v'],\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "\n",
    "base_blip.language_model.decoder = get_peft_model(base_blip.language_model.decoder, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c658544",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(base_blip) # 2,939,964,164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42437964",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load(target_weight, map_location=device)\n",
    "base_blip.load_state_dict(state['model'])\n",
    "base_blip.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d144d6a",
   "metadata": {},
   "source": [
    "### Convert to Torchscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf076e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BridgeInstructblip(nn.Module):\n",
    "    def __init__(self, base_blip):\n",
    "        super().__init__()\n",
    "        self.base_blip = base_blip\n",
    "        \n",
    "    def forward(self, \n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        qformer_input_ids,\n",
    "        qformer_attention_mask,\n",
    "        pixel_values,\n",
    "        decoder_input_ids\n",
    "    ):\n",
    "        \n",
    "        return self.base_blip(\n",
    "            input_ids              = input_ids,\n",
    "            attention_mask         = attention_mask,\n",
    "            qformer_input_ids      = qformer_input_ids,\n",
    "            qformer_attention_mask = qformer_attention_mask,\n",
    "            pixel_values           = pixel_values,\n",
    "            decoder_input_ids      = decoder_input_ids\n",
    "        ).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4942fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge = BridgeInstructblip(base_blip)\n",
    "bridge = bridge.to(device)\n",
    "\n",
    "count_parameters(bridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d674e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example kwarg inputs\n",
    "instructions = f'Question: {\"\"} Options: {\" \".join([f\"({chr(i+97)}) {c}\" for i, c in enumerate([])])} Short answer:'\n",
    "inputs = {\n",
    "    **processor(\n",
    "    images=Image.open(f'competition/train_input_images/TRAIN_000.jpg'),\n",
    "    text=instructions,\n",
    "    return_tensors=\"pt\",\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    ).to(device), \n",
    "    'decoder_input_ids' : torch.full((1, 1), 0, dtype=torch.long).to(device)\n",
    "}\n",
    "\n",
    "# TracerWarning 은 InstructBlipForConditionalGeneration 내에서 예상과 다른 입력값 예외 처리를 위해 if 문을 사용하기 때문입니다.\n",
    "\n",
    "torch_script_model = torch.jit.trace(bridge, example_kwarg_inputs=inputs)\n",
    "torch_script_model.save('checkpoint/torchscript.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
