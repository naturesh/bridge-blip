{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "246fba6b",
   "metadata": {},
   "source": [
    "### Colab Setting\n",
    "- torchscript.ipynb, torchscript_inference.ipynb 의 device 와 동일하게 설정 필수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61941eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "visual7w_path = '/content/visual7w'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80939de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /content/visual7w\n",
    "!wget -P /content/visual7w http://vision.stanford.edu/yukezhu/visual7w_images.zip\n",
    "!wget -P /content/visual7w https://ai.stanford.edu/~yukez/papers/resources/dataset_v7w_telling.zip\n",
    "!unzip -nq /content/visual7w/visual7w_images   -d /content/visual7w\n",
    "!rm /content/visual7w/visual7w_images.zip\n",
    "!unzip -nq /content/visual7w/dataset_v7w_telling -d /content/visual7w\n",
    "!mv /content/visual7w/*telling*.json /content/visual7w/dataset_v7w_telling.json\n",
    "!mv /content/visual7w/visual7w_images /content/visual7w/images\n",
    "!rm /content/visual7w/dataset_v7w_telling.zip\n",
    "\n",
    "%cd /content/drive/MyDrive/bridgeblip\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f206422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import InstructBlipForConditionalGeneration, InstructBlipProcessor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import uniform_filter1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6458cf02",
   "metadata": {},
   "source": [
    "### seed setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5064be",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f1280",
   "metadata": {},
   "source": [
    "### Create Visual7W Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bbf637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframe = []\n",
    "\n",
    "with open(visual7w_path + '/dataset_v7w_telling.json', 'r') as f:\n",
    "    images = json.load(f)['images']\n",
    "\n",
    "    for i in images:\n",
    "        qa_pairs = i['qa_pairs']\n",
    "\n",
    "        for q in qa_pairs:\n",
    "            image    = q['image_id']\n",
    "            question = q['question']\n",
    "            choices  = [*q['multiple_choices'],q['answer']]\n",
    "\n",
    "            random.shuffle(choices)\n",
    "\n",
    "            answer_idx = choices.index(q['answer'])\n",
    "\n",
    "            label = [0, 0, 0, 0]\n",
    "            label[answer_idx] = 1\n",
    "\n",
    "            dataframe.append({\n",
    "                'image_id' : image,\n",
    "                'question' : question,\n",
    "                'choices'  : choices,\n",
    "                'label'    : label\n",
    "            })\n",
    "\n",
    "dataframe = pd.DataFrame(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8439595",
   "metadata": {},
   "source": [
    "### Create **LoRA-Bridge** InstructBlip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f'total params      : {total_params:,}')\n",
    "    print(f'trainable params  : {trainable_params:,}')\n",
    "    print(f'trainable percent : {100. * trainable_params / total_params:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f7069",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout):\n",
    "        super().__init__()\n",
    "        self.layer= nn.Sequential(\n",
    "            nn.Linear(in_features, in_features),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features, out_features)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\")\n",
    "base_blip = InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\")\n",
    "\n",
    "# freeze all params\n",
    "for param in base_blip.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# change language_model head\n",
    "base_blip.language_model.lm_head = ClassificationHead(\n",
    "    in_features=base_blip.language_model.config.d_model,\n",
    "    out_features=4,\n",
    "    dropout=0.0\n",
    ")\n",
    "for param in base_blip.language_model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# slice decoder layer 24 to 8\n",
    "base_blip.language_model.decoder.block = nn.ModuleList([\n",
    "    base_blip.language_model.decoder.block[i] for i in [0, 3, 6, 9, 12, 15, 18, 21]\n",
    "])\n",
    "\n",
    "# re-connect with lora bridge\n",
    "lora_config = LoraConfig(\n",
    "    r=96,\n",
    "    lora_alpha=192,\n",
    "    target_modules=['q', 'k', 'v'],\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "base_blip.language_model.decoder = get_peft_model(base_blip.language_model.decoder, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65794d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(base_blip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b007d66",
   "metadata": {},
   "source": [
    "### Create Visual7W Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d5787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual7W(Dataset):\n",
    "   def __len__(self):\n",
    "      return len(dataframe)\n",
    "   \n",
    "   def __getitem__(self, idx):\n",
    "      row = dataframe.iloc[idx]\n",
    "\n",
    "      image = Image.open(f'{visual7w_path}/images/v7w_{row[\"image_id\"]}.jpg').convert(\"RGB\")\n",
    "\n",
    "      instructions = f'Question: {row[\"question\"]} Options: {\" \".join([f\"({chr(i+97)}) {c}\" for i, c in enumerate(row[\"choices\"])])} Short answer:' # instructions from InstructBLIP paper\n",
    "      inputs = processor(\n",
    "         images=image,\n",
    "         text=instructions,\n",
    "         return_tensors=\"pt\",\n",
    "         padding='max_length',\n",
    "         truncation=True,\n",
    "         max_length=128,\n",
    "      )\n",
    "      return {\n",
    "         'inputs' : inputs,\n",
    "         'label'  : torch.tensor(row['label'], dtype=torch.float) \n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9660d7",
   "metadata": {},
   "source": [
    "## **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f805edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, opt, criterion, device, start_token_id):\n",
    "    model.train()\n",
    "\n",
    "    loss_history = []\n",
    "    bar = tqdm(loader)\n",
    "\n",
    "    for batch in bar:\n",
    "\n",
    "        inputs = { k:v.squeeze(1) for k, v in batch['inputs'].to(device).items() }\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        batch_size = labels.shape[0]\n",
    "        decoder_input_ids = torch.full((batch_size, 1), start_token_id, dtype=torch.long).to(device)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = model(**inputs, decoder_input_ids=decoder_input_ids).logits.squeeze(1)\n",
    "        target = torch.argmax(labels, dim=1)\n",
    "\n",
    "        loss = criterion(logits, target)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "\n",
    "        bar.set_postfix(\n",
    "            loss      = f'{loss.item():.4f}',\n",
    "        )\n",
    "        loss_history.append(loss.item())\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ed272",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_blip   = base_blip.to(device)\n",
    "num_epochs  = 1\n",
    "dataset     = Visual7W()\n",
    "dataloader  = DataLoader(dataset, batch_size=12, shuffle=True)\n",
    "\n",
    "optimizer   = torch.optim.AdamW([\n",
    "    {'params': base_blip.language_model.decoder.parameters(), 'lr': 5e-5},\n",
    "    {'params': base_blip.language_model.lm_head.parameters(), 'lr': 1e-4}\n",
    "])\n",
    "\n",
    "criterion   = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2abf1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(num_epochs):\n",
    "    history = train_epoch(base_blip, dataloader, optimizer, criterion, device, base_blip.language_model.config.decoder_start_token_id)\n",
    "\n",
    "    state = {\n",
    "        \"model\": base_blip.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"epoch\": ep,\n",
    "    }\n",
    "    \n",
    "    torch.save(state, f\"checkpoint/weights-{ep}.pt\")\n",
    "\n",
    "    plt.plot(uniform_filter1d(history, size=100))\n",
    "    plt.ylim(0.3, 1.5)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
